{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16dfe836-9829-42a6-9b11-248bc72b0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('traindata.csv')\n",
    "df['comment'] = df['comment'].astype(str)\n",
    "\n",
    "# 替换任何缺失的评论（如果存在）\n",
    "df['comment'] = df['comment'].fillna('')\n",
    "# 分词器加载\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# 将数据拆分为训练集和验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['comment'], df['label'], test_size=0.2)\n",
    "\n",
    "# 数据编码\n",
    "def encode(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode(train_texts)\n",
    "val_encodings = encode(val_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6bd856c-0f34-499f-8404-c1c964d46798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    20.703688\n",
      "1    19.371219\n",
      "2    30.048977\n",
      "3     7.512244\n",
      "4    17.970326\n",
      "5     4.393547\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['label'].value_counts().reindex(range(6), fill_value=0)\n",
    "\n",
    "label_percentages = (label_counts / label_counts.sum()) * 100\n",
    "\n",
    "print(label_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c337aa9c-7cac-489b-87ee-f9b52225caa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135356166\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7c4964-4842-4485-9467-bd9d9e81d81f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "/share/pkg.8/academic-ml/fall-2024/install/fall-2024-pyt/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERT_Transformer_Model(nn.Module):\n",
    "    def __init__(self, transformer_hidden_dim=128, num_classes=6):\n",
    "        super(BERT_Transformer_Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        \n",
    "        # # 冻结 BERT 的所有参数\n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=768, nhead=8),\n",
    "            num_layers=6\n",
    "        )\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取 BERT 的输出\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        \n",
    "        # Transformer处理\n",
    "        transformer_out = self.transformer(sequence_output)\n",
    "        \n",
    "        # 取Transformer最后一个时间步的输出\n",
    "        transformer_out = transformer_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层输出\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "model=BERT_Transformer_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28824d01-dfc3-4504-9467-0319b1d4ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 构建训练数据和验证数据\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels.values))\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels.values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732a67cd-38b1-444d-93b8-56599f634d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 0.7804, Training accuracy: 0.7213\n",
      "Validation loss: 0.6180, Validation accuracy: 0.7764\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch+1}, Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab567b31-ba7e-40ff-9313-c1f7a23d3326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 TRAN_BERT_FREEZE_01/bert_freeze_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义保存路径\n",
    "save_dir = \"TRAN_BERT_FREEZE_01\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 保存模型权重\n",
    "model_save_path = os.path.join(save_dir, \"bert_freeze_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# 如果需要保存优化器的状态字典（例如恢复训练时使用）\n",
    "optimizer_save_path = os.path.join(save_dir, \"optimizer.pth\")\n",
    "torch.save(optimizer.state_dict(), optimizer_save_path)\n",
    "\n",
    "print(f\"模型已保存到 {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29199a14-f3bc-4fb1-a0cd-573c29f8384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BERT_Transformer_Model(nn.Module):\n",
    "    def __init__(self, transformer_hidden_dim=128, num_classes=6):\n",
    "        super(BERT_Transformer_Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        \n",
    "        # 冻结 BERT 的所有参数\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=768, nhead=8),\n",
    "            num_layers=6\n",
    "        )\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取 BERT 的输出\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        \n",
    "        # Transformer处理\n",
    "        transformer_out = self.transformer(sequence_output)\n",
    "        \n",
    "        # 取Transformer最后一个时间步的输出\n",
    "        transformer_out = transformer_out[:, -1, :]\n",
    "        \n",
    "        # 全连接层输出\n",
    "        logits = self.fc(transformer_out)\n",
    "        return logits\n",
    "\n",
    "model=BERT_Transformer_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac230db-edf7-4ae1-b0c1-c1eec36aabf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1349263.1.academic-gpu/ipykernel_145563/3013386396.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型权重已成功加载！\n"
     ]
    }
   ],
   "source": [
    "# 请在这里初始化你的 BERT 模型\n",
    "optimizer = AdamW(model.fc.parameters(), lr=2e-5)  # 请在这里初始化你的优化器\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "print(\"模型权重已成功加载！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bf51a3-8191-48e6-8d3c-3066193f06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32257768-121e-48fe-a7c7-1e0d84d5a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 0.4825, Training accuracy: 0.8361\n",
      "Validation loss: 0.6155, Validation accuracy: 0.7794\n",
      "Epoch 2, Training loss: 0.4712, Training accuracy: 0.8377\n",
      "Validation loss: 0.6178, Validation accuracy: 0.7791\n",
      "Epoch 3, Training loss: 0.4648, Training accuracy: 0.8392\n",
      "Validation loss: 0.6209, Validation accuracy: 0.7775\n",
      "Epoch 4, Training loss: 0.4635, Training accuracy: 0.8399\n",
      "Validation loss: 0.6233, Validation accuracy: 0.7789\n",
      "Epoch 5, Training loss: 0.4619, Training accuracy: 0.8415\n",
      "Validation loss: 0.6231, Validation accuracy: 0.7805\n",
      "Epoch 6, Training loss: 0.4598, Training accuracy: 0.8415\n",
      "Validation loss: 0.6248, Validation accuracy: 0.7811\n",
      "Epoch 7, Training loss: 0.4586, Training accuracy: 0.8432\n",
      "Validation loss: 0.6251, Validation accuracy: 0.7803\n",
      "Epoch 8, Training loss: 0.4604, Training accuracy: 0.8404\n",
      "Validation loss: 0.6255, Validation accuracy: 0.7814\n",
      "Epoch 9, Training loss: 0.4617, Training accuracy: 0.8408\n",
      "Validation loss: 0.6240, Validation accuracy: 0.7809\n",
      "Epoch 10, Training loss: 0.4599, Training accuracy: 0.8409\n",
      "Validation loss: 0.6250, Validation accuracy: 0.7809\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 假设有以下初始化：\n",
    "# model, optimizer, criterion, train_dataloader, val_dataloader, device, epochs\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "            # Calculate training accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch {epoch+1}, Training loss: {avg_train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# 绘制训练过程中的损失和准确率\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制准确率曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e390232a-b69c-424f-8fd9-8a1bfe00c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('testdata_with_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11a6f185-5597-4806-9171-8a078ff477cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['content'] = test_df['content'].astype(str)\n",
    "test_df['content'] = test_df['content'].fillna('')\n",
    "\n",
    "# 数据编码\n",
    "test_encodings = encode(test_df['content'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed77b318-906c-452d-8d58-af2c10163faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask = [b.to(device) for b in batch]\n",
    "        \n",
    "        # 获取模型的输出\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # 获取预测标签\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# 解码预测的标签回原始标签\n",
    "test_df['half_freeze_label'] = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# 保存结果到文件\n",
    "test_df.to_csv('testdata_with_predictions2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1505f38-7f5f-4385-8110-1b22047f61d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>文件夹</th>\n",
       "      <th>文件名</th>\n",
       "      <th>content</th>\n",
       "      <th>nofreeze_label</th>\n",
       "      <th>half_freeze_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1029</td>\n",
       "      <td>15.txt</td>\n",
       "      <td>激动</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1029</td>\n",
       "      <td>15.txt</td>\n",
       "      <td>#从神五到神十九#</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1029</td>\n",
       "      <td>15.txt</td>\n",
       "      <td>#神十九出征仪式#</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1029</td>\n",
       "      <td>15.txt</td>\n",
       "      <td>#中国空间站我们90后来了#</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1029</td>\n",
       "      <td>15.txt</td>\n",
       "      <td>#神十九航天员的出征仪式好可爱#</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36014</th>\n",
       "      <td>1109</td>\n",
       "      <td>30.txt</td>\n",
       "      <td>[好运连连]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36015</th>\n",
       "      <td>1109</td>\n",
       "      <td>30.txt</td>\n",
       "      <td>这次又是什么原因……怎么感觉它三天两头崩一次</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36016</th>\n",
       "      <td>1109</td>\n",
       "      <td>30.txt</td>\n",
       "      <td>谁给我指个路、、又谁在被保护</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36017</th>\n",
       "      <td>1109</td>\n",
       "      <td>30.txt</td>\n",
       "      <td>怎么又崩了？干啥了</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36018</th>\n",
       "      <td>1109</td>\n",
       "      <td>30.txt</td>\n",
       "      <td>崩了就崩了，干脆直接下架算了</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36019 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        文件夹     文件名                 content  nofreeze_label  half_freeze_label\n",
       "0      1029  15.txt                      激动               1                  1\n",
       "1      1029  15.txt               #从神五到神十九#               0                  0\n",
       "2      1029  15.txt               #神十九出征仪式#               1                  0\n",
       "3      1029  15.txt          #中国空间站我们90后来了#               1                  1\n",
       "4      1029  15.txt        #神十九航天员的出征仪式好可爱#               1                  1\n",
       "...     ...     ...                     ...             ...                ...\n",
       "36014  1109  30.txt                  [好运连连]               1                  1\n",
       "36015  1109  30.txt  这次又是什么原因……怎么感觉它三天两头崩一次               4                  4\n",
       "36016  1109  30.txt          谁给我指个路、、又谁在被保护               2                  2\n",
       "36017  1109  30.txt               怎么又崩了？干啥了               2                  3\n",
       "36018  1109  30.txt          崩了就崩了，干脆直接下架算了               2                  2\n",
       "\n",
       "[36019 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f03398-e0cf-4a22-900c-db0274e19e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f679a-92d3-420e-aa69-a989f14dcb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384548f-168a-4071-975a-f751936c3500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a29b6e-f3d1-49fc-a7af-a6d235bf2019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe21ae-b313-4505-a08e-ae1c28271f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fd0d6-92ae-4afc-b1b1-1abaef687043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7514062-8854-4e3b-8859-0e129f835619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e05f4-0c06-4b33-8dc4-3c3c9a1ee241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4a46d-e1ec-4373-8c79-340226fb7804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89107919-f128-4b41-afbe-c1a5d585a97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95571520-820e-4496-a447-31f499d8a3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ed632-778e-4009-aa51-f1cbb31795bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7da04-b622-4ace-a5c5-b7328263cf55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f10f46-07be-4149-b3db-238f74476c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49479a4c-4179-4be9-b899-7c68d5b25570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c1d4d-4369-44a8-a6d4-996353126a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be71c9-1946-4580-8f3a-61a8fee5a7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2b856-6de6-4960-9b6c-027f0b975a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbb3ea-5a0d-4d91-9790-98b191e5d903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e9bf5-233c-43f5-ad04-80ab2bc56678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
